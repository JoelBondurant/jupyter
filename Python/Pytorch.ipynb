{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afb9bd2-e104-4cd9-903c-cc5914003c8c",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abc6c33f-737a-47b1-bde3-53c4f2f9726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d77727c0-b52e-4956-beeb-293c7f879e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.0+cu121', False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f5d7159-a49d-4021-8424-081ae58a125b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9996, 0.9401],\n",
       "        [0.0683, 0.6881]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.rand(2, 2)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b612cf-37ee-4365-b738-93f012e4b048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6236)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.det()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30fb73f3-aedd-4ef2-98e7-a2233bde32db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[-0.9222, -0.3868],\n",
       "        [-0.3868,  0.9222]]),\n",
       "S=tensor([1.4775, 0.4220]),\n",
       "V=tensor([[-0.6418, -0.7669],\n",
       "        [-0.7669,  0.6418]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65b0d18d-07eb-4707-8389-e6424419f986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
       "\n",
       "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
       "\n",
       "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
       "\n",
       "Args:\n",
       "    in_features: size of each input sample\n",
       "    out_features: size of each output sample\n",
       "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
       "        Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
       "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
       "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
       "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
       "\n",
       "Attributes:\n",
       "    weight: the learnable weights of the module of shape\n",
       "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
       "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
       "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
       "            If :attr:`bias` is ``True``, the values are initialized from\n",
       "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Linear(20, 30)\n",
       "    >>> input = torch.randn(128, 20)\n",
       "    >>> output = m(input)\n",
       "    >>> print(output.size())\n",
       "    torch.Size([128, 30])\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.python/lib/python3.11/site-packages/torch/nn/modules/linear.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cad294-a3cf-477d-8feb-db10ef1fb5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
